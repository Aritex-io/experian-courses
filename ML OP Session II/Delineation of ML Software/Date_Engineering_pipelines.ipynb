{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Date Engineering pipelines.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data: Engineering Pipelines"
      ],
      "metadata": {
        "id": "1N-cxaaXRqw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We mentioned previously that the fundamental part of any machine learning workflow is **Data**. Collecting good data sets has a huge impact on the quality and performance of the ML model. The famous citation <br>\n",
        "                        *“Garbage In, Garbage Out”,*\n",
        "\n",
        "in the machine learning context means that the ML model is only as good as your data. Therefore, the data, which has been used for training of the ML model, indirectly influence the overall performance of the production system. The amount and quality of the data set are usually problem-specific and can be empirically discovered.\n",
        "\n",
        "Being an important step, data engineering is reported as heavily time-consuming. We might spend the majority of time on a machine learning project constructing data sets, cleaning, and transforming data.\n",
        "\n",
        "The data engineering pipeline includes a sequence of operations on the available data. The final goal of these operations is to create training and testing datasets for the ML algorithms. In the following, we describe each stage of the data engineering pipeline such as *Data Ingestion, Exploration and Validation, Data Wrangling (Cleaning), and Data Splitting*."
      ],
      "metadata": {
        "id": "ff4UNXg7R90P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Ingestion"
      ],
      "metadata": {
        "id": "m6cByPmISj4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Ingestion – Collecting data by using various systems, frameworks and formats, such as internal/external databases, data marts, OLAP cubes, data warehouses, OLTP systems, Spark, HDFS etc. This step might also include synthetic data generation or data enrichment The best practices for this step include the following actions that should be maximally automated:\n",
        "\n",
        "    a)Data Sources Identification: Find the data and document its origin (data provenance).\n",
        "    b)Space Estimation: Check how much storage space it will take.\n",
        "    c)Space Location: Create a workspace with enough storage space.\n",
        "    d)Obtaining Data: Get the data and convert them to a format that can be easily manipulated without changing the data itself.\n",
        "    e)Back up Data: Always work on a copy of the data and keep the original dataset untouched.\n",
        "    f)Privacy Compliance: Ensure sensitive information is deleted or protected (e.g., anonymized) to ensure GDPR compliance.\n",
        "    g)Metadata Catalog: Start documenting the metadata of the dataset by recording the basic information about the size, format, aliases, last modified time, and access control lists.\n",
        "    e)Test Data: Sample a test set, put it aside, and never look at it to avoid the “data snooping” bias. You fell for this if you are selecting a particular kind of ML model by using the test set. This will lead to an ML model selection that is too optimistic and will not perform well in production.\n"
      ],
      "metadata": {
        "id": "8S7TQyKOSlFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exploration and Validation"
      ],
      "metadata": {
        "id": "53SbybN-g7R_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploration and Validation – Includes data profiling to obtain information about the content and structure of the data. The output of this step is a set of metadata, such as max, min, avg of values. Data validation operations are user-defined error detection functions, which scan the dataset to spot some errors. The validation is a process of assessing the quality of the data by running dataset validation routines (error detection methods). For example, for “address”-attributes, are the address components consistent? Is the correct postal code associated with the address? Are there missing values in the relevant attributes? The best practices for this step include the following actions:\n",
        "\n",
        "    a)Use RAD tools: Using Jupyter notebooks is a good way to keep records of data exploration and experimentation.\n",
        "    b)Attribute Profiling: Obtain and document the metadata about each attribute, such as\n",
        "         a) Name\n",
        "         b) Number of Records\n",
        "         c) Data Type (categorical, numerical, int/float, text, structured, etc.\n",
        "         d) Numerical Measures (min, max, avg, median, etc. for numerical data)\n",
        "         e) Amount of missing values (or “missing value ratio” = Number of absent values/ Number of records)\n",
        "         f) Type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
        "    c) Label Attribute Identification: For supervised learning tasks, identify the target attribute(s).\n",
        "    d) Data Visualization: Build a visual representation for value distribution.\n",
        "    e) Attributes Correlation: Compute and analyze the correlations between attributes.\n",
        "    f) Additional Data: Identify data that would be useful for building the model (go back to “Data Ingestion”).\n",
        "\n"
      ],
      "metadata": {
        "id": "gJHRYSTlhBbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Wrangling (Cleaning)"
      ],
      "metadata": {
        "id": "py6Tm2OyiTSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Wrangling (Cleaning) – Data preparation step where we programmatically wrangle data, e.g., by re-formatting or re-structuring particular attributes that might change the form of the data’s schema. We recommend writing scripts or functions for all data transformations in the data pipeline to re-use all these functionalities on future data."
      ],
      "metadata": {
        "id": "SfPdaOx6iYbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Transformations: Identify the promising transformations you may want to apply.\n",
        "*   Outliers: Fix or remove outliers (optional).\n",
        "*   Outliers: Fix or remove outliers (optional).\n",
        "*   Not relevant Data: Drop the attributes that provide no useful information for the task (relevant for feature engineering).\n",
        "*   Restructure Data: Might include the following operations\n",
        "\n",
        "          a) Reordering record fields by moving columns\n",
        "          b) Creating new record fields through extracting values\n",
        "          c) Combining multiple record fields into a single record field\n",
        "          d) Filtering datasets by removing sets of records\n",
        "          e) Shifting the granularity of the dataset and the fields associated with records through aggregations and pivots.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gQnCU_hMia1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Splitting"
      ],
      "metadata": {
        "id": "ycGK5lPajJvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting – Splitting the data into training (80 %), validation, and test datasets to be used during the core machine learning stages to produce the ML model."
      ],
      "metadata": {
        "id": "GLiDJfE9jMFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7L31a0kRqEX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}